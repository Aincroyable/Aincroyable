{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec217543-7d2b-49b5-b485-1a90f37c8abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import polars as pl\n",
    "from sklearn.manifold import TSNE\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03f8d6c-1117-4132-8379-73d8682e96a0",
   "metadata": {},
   "source": [
    "# read lexique database  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06475936-c219-4213-8ae1-b3d71c63b20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can download it from http://www.lexique.org/\n",
    "lexique = pd.read_csv(\"\\\\Lexique383.tsv\",sep='\\t')\n",
    "# select only words containing the sound /ɛ̃/\n",
    "sub = lexique.loc[lexique['phon'].str.contains('5')].drop_duplicates('lemme').reset_index(drop=True)\n",
    "words = sub['ortho'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96755111-d331-4082-9815-58fdefe2a668",
   "metadata": {},
   "source": [
    "# Get emotionnal valence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e35d15d-380e-4145-8654-7c126847eb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model from google https://ai.google.dev/gemini-api?hl=fr\n",
    "genai.configure(api_key=\"put here your api key\")\n",
    "model = genai.GenerativeModel('gemini-1.5-flash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75143aea-52a3-457d-8939-feb7dcb02945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine prompt based on this recent paper\n",
    "#https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=Could+large+language+models+estimate+valence+of+words%3F+A+small+ablation+study.+Proceedings+of+CBIC.&btnG=\n",
    "\n",
    "prompt = \"Dans quelle mesure ce mot est-il négatif ou positif sur une échelle de 1 à 9 ? Réponds uniquement avec un chiffre, 1 étant « très négatif » et 9 « très positif ». Voici le mot : \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cc9d8d-0a07-4b2b-b138-fb61fabc4874",
   "metadata": {},
   "outputs": [],
   "source": [
    "valences = []\n",
    "fails_mots = []\n",
    "fails_defs = []\n",
    "fails_idxs = []\n",
    "for num,mot in tqdm(enumerate(mots)):\n",
    "    response = model.generate_content(prompt+str(mot))\n",
    "    try:\n",
    "        valences.append(re.findall(r'\\d+', response.text)[0])\n",
    "    except:\n",
    "        fails_mots.append(mot)\n",
    "        fails_defs.append(response)\n",
    "        fails_idxs.append(num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1ad595-eeaf-49a2-8c6e-1cfa08fe6d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as a pickle file\n",
    "moby = { \"valences\": valences,\n",
    "        \"fails_mots\": fails_mots,\n",
    "        \"fails_defs\": fails_defs,\n",
    "        \"fails_idxs\": fails_idxs}\n",
    "with open(\"valences.pickle\", 'wb') as handle:\n",
    "    pickle.dump(moby, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52da90d-f125-4a02-9ac6-aab829fbbd30",
   "metadata": {},
   "source": [
    "# Get example sentences to compute embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac77d7b-e1a5-49ab-b344-b3773115984d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create full database based on the sentences taken from https://huggingface.co/datasets/La-matrice/french_sentences_19M\n",
    "# we will then select all the sentences that contain the selected words to be able to compute each word averaged embedding\n",
    "path = \"\\\\sentences_dataset\\\\\"\n",
    "files = [\"0000.parquet\",\"0001.parquet\",\"0002.parquet\",\"0003.parquet\",\"0004.parquet\"]\n",
    "database = []\n",
    "for file in files:\n",
    "    dd = pd.read_parquet(path+file, engine='pyarrow')\n",
    "    database.append(dd)\n",
    "finaldatabase = pd.concat(database)\n",
    "# remove duplicates\n",
    "finaldatabase = finaldatabase.drop_duplicates('text').reset_index(drop=True)\n",
    "finaldatabase['text'] = ' '+finaldatabase['text']+' '\n",
    "# create polars dataframe for speed efficiency \n",
    "dfi =  pl.from_dataframe(finaldatabase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba27695-09f5-4225-95a0-123e25d55af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quite time consumming (~ 1 hour)\n",
    "\n",
    "total_indexes = []\n",
    "total_text = []\n",
    "total_word = []\n",
    "outs = []\n",
    "\n",
    "for curname in tqdm(words):\n",
    "\n",
    "    mask = dfi.select(pl.col(\"text\").str.contains(curname))\n",
    "    curdat = finaldatabase[np.array(mask['text'].to_list())]\n",
    "    \n",
    "    #curdat = finaldatabase.loc[finaldatabase['text'].str.contains(curname)]\n",
    "    if len(curdat)==0:\n",
    "        outs.append(curname)\n",
    "    else:\n",
    "        naames = np.repeat(curname,len(curdat))\n",
    "        iid = np.arange(1,len(curdat)+1)\n",
    "        total_indexes.append(iid)\n",
    "        total_text.append(curdat['text'].values)\n",
    "        total_word.append(naames)\n",
    "\n",
    "dico = {\n",
    "    \n",
    "    \"name\": np.concatenate(total_word),\n",
    "    \"individual_index\": np.concatenate(total_indexes), \n",
    "    \"sentence\": np.concatenate(total_text)\n",
    "    \n",
    "}\n",
    "dff = pd.DataFrame.from_dict(dico)\n",
    "dff.to_csv('multiple_sentences_for_embeddings.csv',index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b494b24-6900-4376-a7d1-6feaaaa56030",
   "metadata": {},
   "source": [
    "# get averaged embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0e841e-904d-4280-b805-8febb829d0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise LLM for embeddings computation\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ee5ae2-c50e-4bb6-9ae5-efa6fd0652b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take only ten of the sentences for fast action! \n",
    "# get averaged embedding for each word based on 10 sentences\n",
    "fails = []\n",
    "mean_embedds = []\n",
    "len_embedds = []\n",
    "whoo = []\n",
    "for word in words:\n",
    "    indiv = data.loc[data['name']==word].reset_index(drop=True)\n",
    "    if len(indiv)>10:\n",
    "        iid = np.arange(0,len(indiv))\n",
    "        indiv = indiv.iloc[np.asarray(random.choices(iid, k =10))]\n",
    "        \n",
    "    momo = indiv['name'].values[0]\n",
    "    \n",
    "    # retrieve token(s) needed\n",
    "    token_ids = []\n",
    "    token_strings = []\n",
    "    tokenizer = model._first_module().tokenizer\n",
    "    \n",
    "    ids = tokenizer.encode(momo)\n",
    "    strings = tokenizer.convert_ids_to_tokens(ids)\n",
    "    token_ids.append(ids)\n",
    "    token_strings.append(strings)\n",
    "    needed = token_strings[0][1:-1]\n",
    "    \n",
    "    fail = []\n",
    "    embs = []\n",
    "    for num in tqdm(range(len(indiv))):\n",
    "        # get each sentence\n",
    "        sent = indiv.iloc[num]['sentence']\n",
    "        # get embedding for each token within each sentence\n",
    "        embeddings = model.encode(sent,output_value = \"token_embeddings\")\n",
    "        token_ids2 = []\n",
    "        token_strings2 = []\n",
    "        ids2 = tokenizer.encode(sent)\n",
    "        strings2 = tokenizer.convert_ids_to_tokens(ids2)\n",
    "        token_ids2.append(ids2)\n",
    "        token_strings2.append(strings2)\n",
    "        # make sure no discrepencies\n",
    "        if (len(embeddings) == len(token_strings2[0])):\n",
    "    \n",
    "            # retrieve Token(s) positions \n",
    "            L = token_strings2[0]; S= needed.copy()\n",
    "            n = len(S)\n",
    "            \n",
    "            for i in range(len(L)-n + 1):\n",
    "                if S== L[i:i + n]:\n",
    "                    start = i\n",
    "                    end = i+n\n",
    "            \n",
    "            if embeddings[start:end,:].shape[0] == len(needed):\n",
    "                if embeddings[start:end,:].shape[0]>1:\n",
    "                    final_emb = np.mean(embeddings.numpy()[start:end,:],0)\n",
    "                else: \n",
    "                    final_emb = embeddings.numpy()[start:end,:]\n",
    "                embs.append(final_emb)\n",
    "            else: \n",
    "                fail.append(num)\n",
    "        else: \n",
    "            fail.append(num)\n",
    "    fails.append(fail)\n",
    "    mean_embedds.append(np.mean(embs,0))\n",
    "    len_embedds.append(len(embs))\n",
    "    whoo.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cfe5a4-1488-4114-bc57-1163435b17f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embedds = np.array(mean_embedds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b5ace0-f607-49a3-b7f1-e6e697a16220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run TSNE to reduce the dimension of the embedding vectors\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "v2d = tsne.fit_transform(all_embedds)\n",
    "# create a dataframe with the words and their unique embedding\n",
    "dic2 = {\n",
    "    'x': v2d2[:, 0],\n",
    "    'y': v2d2[:, 1],\n",
    "    'names': whoo}\n",
    "\n",
    "datadf2 = pd.DataFrame.from_dict(dic2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
